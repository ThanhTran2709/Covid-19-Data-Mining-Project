{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from thefuzz import process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Lets use fuzzy matching to create a mapping between the country names in the hospital beds dataset and the location dataset\n",
    "from thefuzz import process\n",
    "\n",
    "import csv\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#from datetime import datetime as dt\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(y_preds, file_name): \n",
    "    with open(file_name, \"w\") as csvfile: \n",
    "        wr = csv.writer(csvfile, quoting=csv.QUOTE_ALL) \n",
    "        wr.writerow([\"Id\", \"Prediction\"]) \n",
    "        for i, pred in enumerate(y_preds): \n",
    "            wr.writerow([str(i), str(pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_train_df = pd.read_csv('data/cases_2021_train.csv')#.dropna(subset=['date_confirmation'])\n",
    "cases_test_df = pd.read_csv('data/cases_2021_test.csv')#.dropna(subset=['date_confirmation'])\n",
    "cases_df = pd.concat([cases_train_df, cases_test_df])\n",
    "locations_df = pd.read_csv('data/location_2021.csv')\n",
    "country_continent_df = pd.read_csv('data/continents-according-to-our-world-in-data.csv')\n",
    "country_continent_df.rename(columns={'Entity':'country', 'Continent':'continent', 'Code':'code'}, inplace=True)\n",
    "country_continent_df = country_continent_df[['country', 'code', 'continent']]\n",
    "pop_df = pd.read_csv('population.csv')\n",
    "pop_df.rename(columns={'Country Name':'country', '2022':'population'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group locations_df by country and sum the values\n",
    "locations_df = locations_df.groupby('Country_Region').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data set containing country, continent pairs and merging with the location data\n",
    "# https://ourworldindata.org/grapher/continents-according-to-our-world-in-data\n",
    "country_continent_df.rename(columns={'Entity':'country', 'Continent':'continent', 'Code':'code'}, inplace=True)\n",
    "country_continent_df = country_continent_df[['country', 'code', 'continent']]\n",
    "country_continent_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for countries that are not in the external data set\n",
    "unique_country_names = pd.DataFrame(pd.unique(locations_df['Country_Region']))\n",
    "unique_country_names.columns = ['country']\n",
    "matches = unique_country_names.merge(country_continent_df, left_on='country', right_on='country', how='left')\n",
    "# check for missing values in continent column\n",
    "print(matches[matches['continent'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a mismatch in the naming scheme between the two datasets. We can use the following mappings to get a better merge.\n",
    "- US -> United States x\n",
    "- Micronesia (Country) -> Micronesia x\n",
    "- Holy See -> Vatican x\n",
    "- Burma -> Myanmar x\n",
    "- Taiwan* -> Taiwan x\n",
    "- Congo (Brazzaville), Congo (Kinshasa)\t-> Congo x\n",
    "- Diamond See -> Cruise x\n",
    "- MS Zaandam -> Cruise\n",
    "- Cabo Verde -> Cabo Verde (This was manually added to the dataset .csv file)\n",
    "- Korea, South -> South Korea x\n",
    "- Timor-Leste -> Timor-Leste(This was manually added to the dataset .csv file)\n",
    "- West Bank and Gaza -> Palestine x\n",
    "\n",
    "We can manually match these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_location_data(df):\n",
    "    df.rename(columns={'Country_Region':'country', 'Province_State':'state', 'Lat':'lat', 'Long':'long'}, inplace=True)\n",
    "    # Cleaning country names\n",
    "    df['country'] = df['country'].str.replace('US', 'United States', regex=False)\n",
    "    df['country'] = df['country'].str.replace('Korea, South', 'South Korea', regex=False)\n",
    "    df['country'] = df['country'].str.replace('Taiwan*', 'Taiwan', regex=False)\n",
    "    df['country'] = df['country'].str.replace('Micronesia (Country)', 'Micronesia', regex=False)\n",
    "    df['country'] = df['country'].str.replace('Holy See', 'Vatican', regex=False)\n",
    "    df['country'] = df['country'].str.replace('West Bank and Gaza', 'Palestine', regex=False)\n",
    "    df['country'] = df['country'].str.replace('Diamond See', 'International Waters', regex=False)\n",
    "    df['country'] = df['country'].str.replace('Congo (Brazzaville)', 'Republic of Congo', regex=False)\n",
    "    df['country'] = df['country'].str.replace('Congo (Kinshasa)', 'Democratic Republic of Congo', regex=False)\n",
    "    df['country'] = df['country'].str.replace('Burma', 'Myanmar', regex=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_country_names = pd.DataFrame(pd.unique(clean_location_data(locations_df)['country']))\n",
    "unique_country_names.columns = ['country']\n",
    "matches = unique_country_names.merge(country_continent_df, left_on='country', right_on='country', how='left')\n",
    "# check for missing values in continent column\n",
    "print(matches[matches['continent'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = clean_location_data(locations_df)\n",
    "# Merging the dataframes\n",
    "locations_df = locations_df.merge(country_continent_df, left_on='country', right_on='country', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df_grouped = cases_df.groupby('country').size()\n",
    "\n",
    "# Now lets merge the dataframes to get the population for each country\n",
    "cases_df_grouped = cases_df_grouped.reset_index()\n",
    "cases_df_grouped.rename(columns={0:'cases'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df_grouped = cases_df_grouped.merge(locations_df, on='country', how='left')\n",
    "cases_df_grouped = cases_df_grouped.merge(pop_df, left_on='country', right_on='country', how='left')[['country','continent','cases','Confirmed','Case_Fatality_Ratio','population']]\n",
    "cases_df_grouped2 = cases_df_grouped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by continent and summing the values\n",
    "cases_df_grouped = cases_df_grouped.groupby('continent').sum().reset_index().sort_values('Confirmed', ascending=False)\n",
    "cases_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a diverging blue and red color map\n",
    "cmap = plt.get_cmap('bwr')\n",
    "colors = cmap(np.linspace(1, 0, len(cases_df_grouped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar chart of the number of cases per continent\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.suptitle('Data Availability per Continent')\n",
    "plt.subplot(121)\n",
    "plt.bar(cases_df_grouped['continent'], cases_df_grouped['Confirmed'] / 10**6, color=colors)\n",
    "plt.title('Number of confirmed cases in locations dataset')\n",
    "plt.ylabel('Confirmed cases, millions')\n",
    "plt.xlabel('Continent')\n",
    "plt.xticks(rotation=45)\n",
    "# Change ytics to scientific notation\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.tight_layout()\n",
    "plt.subplot(122)\n",
    "plt.bar(cases_df_grouped['continent'], cases_df_grouped['cases'] / 10**6, color=colors)\n",
    "plt.title('Number of cases in cases dataset')\n",
    "plt.xlabel('Continent')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ticklabel_format(style='scientific', axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/continent_cases.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df_grouped2['data_availability_confirmed'] = cases_df_grouped2['cases'] / cases_df_grouped2['Confirmed']* 100\n",
    "cases_df_grouped2['data_availability_population'] = cases_df_grouped2['cases'] / cases_df_grouped2['population'] * 100\n",
    "# Replace Nan values with 0\n",
    "cases_df_grouped2.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(cases_df_grouped2)\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Merging the geodataframe with the population dataframe\n",
    "world = world.merge(gdf, left_on='name', right_on='country', how='left')\n",
    "\n",
    "# Plotting the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "world.boundary.plot(ax=ax)\n",
    "world.plot(column='data_availability_population', ax=ax, legend=True, cmap='viridis')\n",
    "plt.title('Data Availability per Country', size=24)\n",
    "# Remove axis\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "# Remove outline\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "# Increase colorbar font size\n",
    "cbar = ax.get_figure().get_axes()[1]\n",
    "cbar.tick_params(labelsize=15)\n",
    "plt.savefig('plots/data_availability_map.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.read_csv('data/cases_2021_train.csv')\n",
    "locations_df = pd.read_csv('data/location_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDateTime(dateTimeSeries):\n",
    "    \n",
    "    if pd.isna(dateTimeSeries):\n",
    "        return dt.date(2000,1,1)\n",
    "    elif (dateTimeSeries == '25.02.2020 - 26.02.2020'):\n",
    "        return dt.date(2020,2,25)\n",
    "    \n",
    "    dateArray = dateTimeSeries.split('.')\n",
    "    \n",
    "    day = int(dateArray[0])\n",
    "    month = int(dateArray[1])\n",
    "    year = int(dateArray[2])\n",
    "\n",
    "    dateVal = dt.date(year, month, day)\n",
    "    return dateVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizeOutcome(outcome):\n",
    "    \n",
    "    outcomeDictionary = {'hospitalized' : 'Hospitalized/Under Treatment', \n",
    "                         'recovered' : 'Recovered/Discharged', \n",
    "                         'deceased' : 'Deceased', \n",
    "                         'discharged' : 'Recovered/Discharged', \n",
    "                         'alive' : 'Recovered/Discharged',\n",
    "                         'discharge' : 'Recovered/Discharged', \n",
    "                         'under treatment' : 'Hospitalized/Under Treatment', \n",
    "                         'stable' : 'Recovered/Discharged', \n",
    "                         'died' : 'Deceased',\n",
    "                         'receiving treatment' : 'Hospitalized/Under Treatment', \n",
    "                         'death' : 'Deceased', \n",
    "                         'stable condition' : 'Recovered/Discharged', \n",
    "                         'dead' : 'Deceased',\n",
    "                         'discharged from hospital' : 'Recovered/Discharged', \n",
    "                         'critical condition' : 'Hospitalized/Under Treatment',\n",
    "                         'released from quarantine' : 'Recovered/Discharged', \n",
    "                         'recovering at home 03.03.2020' : 'Recovered/Discharged'}\n",
    "    \n",
    "    return outcomeDictionary.get(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertPosition2Rad(latitude,longitude):\n",
    "    conversion = np.deg2rad([latitude,longitude])\n",
    "    return conversion[0], conversion[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAge(rawAge):\n",
    "    if (pd.isna(rawAge)):\n",
    "        return np.nan\n",
    "    \n",
    "    if (len(rawAge) <= 2):\n",
    "        return int(rawAge)\n",
    "    elif '-' not in rawAge:\n",
    "        return round(float(rawAge))\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipelinePreprocess(case_df, location_df):\n",
    "    # Merge primary dataset w/ provincial dataset\n",
    "    preprocessed_df = pd.merge(left=case_df, right=location_df, left_on=['country', 'province'], right_on=['Country_Region', 'Province_State'], how='left')\n",
    "    # Clean up column names\n",
    "    preprocessed_df = preprocessed_df.rename(columns={'Lat':'province_latitude', 'Long_':'province_longitude', 'Confirmed':'confirmed', 'Deaths':'deaths', \n",
    "                            'Recovered':'recovered', 'Active':'active', 'Combined_Key':'combined_key', 'Incident_Rate':'incident_rate', \n",
    "                            'Case_Fatality_Ratio':'Expected_Mortality_Rate'})\n",
    "    # Select relevant columns\n",
    "    preprocessed_df = preprocessed_df[['age', 'sex', 'province', 'country', 'latitude', 'longitude', 'date_confirmation', 'source','chronic_disease_binary', 'outcome', \n",
    "             'province_latitude', 'province_longitude', 'confirmed', 'deaths', 'recovered', 'active','combined_key', 'incident_rate', 'Expected_Mortality_Rate']]\n",
    "    # Categorize outcomes into the most salient\n",
    "    preprocessed_df['outcome'] = preprocessed_df['outcome'].str.lower()\n",
    "    preprocessed_df['outcome'] = preprocessed_df['outcome'].apply(lambda x: categorizeOutcome(x))\n",
    "    # Cleanup date_confirmation\n",
    "    preprocessed_df['date_confirmation'] = preprocessed_df.apply(lambda x: processDateTime(x['date_confirmation']), axis=1)\n",
    "    # Convert coordinates from degrees to radians\n",
    "    preprocessed_df['latitude_r'] = preprocessed_df['latitude'].apply(lambda x: np.deg2rad(x))\n",
    "    preprocessed_df['longitude_r'] = preprocessed_df['longitude'].apply(lambda x: np.deg2rad(x))\n",
    "    preprocessed_df['province_latitude_r'] = preprocessed_df['province_latitude'].apply(lambda x: np.deg2rad(x))\n",
    "    preprocessed_df['province_longitude_r'] = preprocessed_df['province_longitude'].apply(lambda x: np.deg2rad(x))\n",
    "\n",
    "    # Process age\n",
    "    preprocessed_df['age'] = preprocessed_df['age'].apply(lambda x: processAge(x))\n",
    "\n",
    "    # Impute missing values using Iterative Imputer\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "    # Impute missing values for age\n",
    "    preprocessed_df['age'] = imputer.fit_transform(preprocessed_df[['age']])\n",
    "    preprocessed_df['age'] = preprocessed_df['age'].apply(lambda x: round(x))\n",
    "    # Map sex to binary\n",
    "    # Mapping sex to a number\n",
    "    sexMap = {'male':0, 'female':1}\n",
    "    preprocessed_df['sex'] = preprocessed_df['sex'].map(sexMap)\n",
    "    # Impute sex\n",
    "    preprocessed_df['sex'] = imputer.fit_transform(preprocessed_df[['sex']])\n",
    "    preprocessed_df['sex'] = preprocessed_df['sex'].apply(lambda x: round(x))\n",
    "    \n",
    "    # One hot encode country and province, then use iterative imputer to fill in missing values\n",
    "    preprocessed_df = pd.get_dummies(preprocessed_df, columns=['country', 'province'])\n",
    "    \n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pipelinePreprocess(cases_df, locations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks 3 - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting external data\n",
    "\n",
    "### Hopital beds (per 10k) -> hospital_beds_per_10k\n",
    "\n",
    "https://www.who.int/data/gho/data/indicators/indicator-details/GHO/hospital-beds-(per-10-000-population)\n",
    "\n",
    "### Population Density and Income -> pop_density_per_sqkm, country_income\n",
    "\n",
    "https://data.worldbank.org/indicator/EN.POP.DNST?type=shaded&view=map&year=2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_o = pd.read_excel('data/cases_2021_train_processed_2.xlsx').dropna(subset=['outcome_group'])\n",
    "test_o = pd.read_excel('data/cases_2021_test_processed_unlabelled_2.xlsx')\n",
    "locations = pd.read_csv('data/location_2021.csv')\n",
    "hospitalBeds = pd.read_excel('data/hospital_beds_per_10k.xlsx', skiprows=2)\n",
    "countryIncome = pd.read_csv('data/country_income.csv')\n",
    "popDensity = pd.read_csv('data/pop_density_per_sqkm.csv', skiprows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipelinePreprocess2(case_df, location_df, hospital_beds_df, country_income_df, pop_density_df, train=True, encoder=None, imputer=None, scaler=None):\n",
    "    \n",
    "    def haversine(lat1, lon1, lat2, lon2, radians=False):\n",
    "        R = 6371\n",
    "        if not radians:\n",
    "            lat1 = np.deg2rad(lat1)\n",
    "            lon1 = np.deg2rad(lon1)\n",
    "            lat2 = np.deg2rad(lat2)\n",
    "            lon2 = np.deg2rad(lon2)\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.deg2rad(lat1)) * np.cos(np.deg2rad(lat2)) * np.sin(dlon/2) * np.sin(dlon/2)\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        d = R * c\n",
    "        return d\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    location_df = location_df.drop(columns=['Last_Update'])\n",
    "    # Drop redundant columns\n",
    "    location_df = location_df.drop(columns=['Recovered', 'Active', 'Incident_Rate', 'Confirmed', 'Deaths', 'Case_Fatality_Ratio'])\n",
    "    # Merge primary dataset w/ provincial dataset\n",
    "    preprocessed_df = pd.merge(left=case_df, right=location_df, left_on=['country', 'province'], right_on=['Country_Region', 'Province_State'], how='left')\n",
    "    # Drop redundant columns\n",
    "    preprocessed_df = preprocessed_df.drop(columns=['Country_Region', 'Province_State'])\n",
    "    # Clean up column names\n",
    "    preprocessed_df = preprocessed_df.rename(columns={'Lat':'province_latitude', 'Long_':'province_longitude', 'Confirmed':'confirmed', 'Deaths':'deaths', \n",
    "                            'Recovered':'recovered', 'Active':'active', 'Combined_Key':'combined_key', 'Incident_Rate':'incident_rate', \n",
    "                            'Case_Fatality_Ratio':'case_fatality_ratio'})\n",
    "    \n",
    "    # Creating a mapping between the country names in the hospital beds dataset and the location dataset with the matching score\n",
    "    # If the score is above some threshold, we will consider it a match\n",
    "    # If the score is better than the previous match, we will replace the previous match\n",
    "    \n",
    "    hospital_country_mapping = {}\n",
    "    for country in pd.unique(case_df['country']):\n",
    "        match, score, _ = process.extractOne(country, hospital_beds_df['Location'])\n",
    "        if score > 86:\n",
    "            if country not in hospital_country_mapping:\n",
    "                hospital_country_mapping[country] = (match, score)\n",
    "            else:\n",
    "                if score > hospital_country_mapping[country][1]:\n",
    "                    hospital_country_mapping[country] = (match, score)\n",
    "        else:\n",
    "            print(f'No match for {country}')\n",
    "\n",
    "    # Add the rest of the countries that did not match\n",
    "    hospital_country_mapping['South Korea'] = ('Republic of Korea', 100)\n",
    "    hospital_country_mapping['San Marino'] = ('Italy', 100)\n",
    "        \n",
    "    # Lets use the mapping to create a new column in the hospital beds dataset that matches the country names in the case_df dataset\n",
    "    hospital_beds_df = hospital_beds_df[['Location', 'Tooltip']].rename(columns={'Location':'country', 'Tooltip':'hospital_beds_per_10k'})\n",
    "    hospital_beds_df['country'] = hospital_beds_df['country'].apply(lambda x: hospital_country_mapping[x][0] if x in hospital_country_mapping else x)\n",
    "\n",
    "    # Merge the hospital beds dataset with the case_df dataset\n",
    "    preprocessed_df = pd.merge(left=preprocessed_df, right=hospital_beds_df, left_on='country', right_on='country', how='left', validate='m:1')\n",
    "\n",
    "    # Doing the same for population density\n",
    "    pop_density_df = pop_density_df[['Country Name', '2019']].rename(columns={'Country Name':'country', '2019':'pop_density_per_sqkm'})\n",
    "\n",
    "    pop_density_mapping = {}\n",
    "    for country in pd.unique(case_df['country']):\n",
    "        match, score, _ = process.extractOne(country, pop_density_df['country'])\n",
    "        if score > 89:\n",
    "            if country not in pop_density_mapping:\n",
    "                pop_density_mapping[country] = (match, score)\n",
    "            else:\n",
    "                if score > pop_density_mapping[country][1]:\n",
    "                    pop_density_mapping[country] = (match, score)\n",
    "        else:\n",
    "            print(f'No match for {country}')\n",
    "\n",
    "    pop_density_mapping['South Korea'] = ('Korea, Rep.', 100)\n",
    "\n",
    "    pop_density_df['country'] = pop_density_df['country'].apply(lambda x: pop_density_mapping[x][0] if x in pop_density_mapping else x)\n",
    "    preprocessed_df = pd.merge(left=preprocessed_df, right=pop_density_df, left_on='country', right_on='country', how='left', validate='m:1')\n",
    "\n",
    "    # Doing the same for country income\n",
    "    country_income_df = country_income_df.rename(columns={'Country':'country', 'IncomeGroup':'income_group'}).drop(columns=['Country Code'])\n",
    "\n",
    "    income_mapping = {}\n",
    "    for country in pd.unique(case_df['country']):\n",
    "        match, score, _ = process.extractOne(country, pop_density_df['country'])\n",
    "        if score > 89:\n",
    "            if country not in income_mapping:\n",
    "                income_mapping[country] = (match, score)\n",
    "            else:\n",
    "                if score > income_mapping[country][1]:\n",
    "                    income_mapping[country] = (match, score)\n",
    "        else:\n",
    "            print(f'No match for {country}')\n",
    "\n",
    "    income_mapping['South Korea'] = ('Korea, Rep.', 100)\n",
    "\n",
    "    country_income_df['country'] = country_income_df['country'].apply(lambda x: income_mapping[x][0] if x in income_mapping else x)\n",
    "    # Mapping the income group to a number\n",
    "    incomeMap = {'Low income':0, 'Lower middle income':1, 'Middle income':2, 'Upper middle income':3, 'High income':4}\n",
    "    country_income_df['income_group'] = country_income_df['income_group'].map(incomeMap)\n",
    "    country_income_df['income_group'] = country_income_df['income_group'].fillna(-1)\n",
    "    preprocessed_df = pd.merge(left=preprocessed_df, right=country_income_df, left_on='country', right_on='country', how='left', validate='m:1')\n",
    "\n",
    "    # If the lat and long are greater than 90 and 180 respectively, we will assume that they are outliers and replace them with the mean of the country\n",
    "    preprocessed_df['latitude'] = preprocessed_df['latitude'].apply(lambda x: x if x < 90 else np.nan)\n",
    "    preprocessed_df['longitude'] = preprocessed_df['longitude'].apply(lambda x: x if x < 180 else np.nan)\n",
    "    preprocessed_df['latitude'] = preprocessed_df['latitude'].fillna(preprocessed_df.groupby('country')['latitude'].transform('mean'))\n",
    "    preprocessed_df['longitude'] = preprocessed_df['longitude'].fillna(preprocessed_df.groupby('country')['longitude'].transform('mean'))\n",
    "    \n",
    "\n",
    "    preprocessed_df['latitude'] = preprocessed_df['latitude'].apply(lambda x: np.deg2rad(x))\n",
    "    preprocessed_df['longitude'] = preprocessed_df['longitude'].apply(lambda x: np.deg2rad(x))\n",
    "    preprocessed_df['province_latitude'] = preprocessed_df['province_latitude'].apply(lambda x: np.deg2rad(x))\n",
    "    preprocessed_df['province_longitude'] = preprocessed_df['province_longitude'].apply(lambda x: np.deg2rad(x))\n",
    "\n",
    "\n",
    "    # Calculate the distance between the case location and the province capital\n",
    "    preprocessed_df['distance_to_province_capital'] = haversine(preprocessed_df['latitude'], preprocessed_df['longitude'], preprocessed_df['province_latitude'], preprocessed_df['province_longitude'], radians=True)\n",
    "\n",
    "    # If the province column is nan, replace the distance_to_province_capital with 0\n",
    "    # Then add a new column called no_province which is 1 if the province is nan and 0 otherwise\n",
    "    def no_province(row):\n",
    "        if pd.isna(row['province']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    preprocessed_df['no_province'] = preprocessed_df.apply(no_province, axis=1)\n",
    "        \n",
    "    # Mapping sex to a number\n",
    "    sexMap = {'male':0, 'female':1}\n",
    "    preprocessed_df['sex'] = preprocessed_df['sex'].map(sexMap)\n",
    "\n",
    "    # One hot encoding province and country\n",
    "    #preprocessed_df = pd.get_dummies(preprocessed_df, columns=['province', 'country'])\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    # Combined key\n",
    "    preprocessed_df = preprocessed_df.drop(columns='combined_key')\n",
    "    # Date confirmation\n",
    "    # We will add the month of the date confirmation as a feature\n",
    "\n",
    "    preprocessed_df['date_confirmation'] = pd.to_datetime(preprocessed_df['date_confirmation'])\n",
    "\n",
    "    preprocessed_df['month_confirmation'] = preprocessed_df['date_confirmation'].dt.month\n",
    "\n",
    "    preprocessed_df = preprocessed_df.drop(columns='date_confirmation')\n",
    "\n",
    "    # Mapping outcome group to a number\n",
    "    if train:\n",
    "        outcomeMap = {'deceased':0, 'hospitalized':1, 'nonhospitalized':2}\n",
    "        preprocessed_df['outcome_group'] = preprocessed_df['outcome_group'].map(outcomeMap)\n",
    "\n",
    "    VAR_NUM = ['age','latitude','longitude','confirmed','deaths','recovered','active','incident_rate',\n",
    "           'case_fatality_ratio', 'province_latitude', 'province_longitude', 'hospital_beds_per_10k', \n",
    "           'pop_density_per_sqkm', 'distance_to_province_capital']\n",
    "\n",
    "    VAR_1HE = ['province', 'country']\n",
    "\n",
    "    VAR_ELSE = ['sex', 'chronic_disease_binary', 'no_province', 'income_group', 'month_confirmation']\n",
    "\n",
    "    if train:\n",
    "\n",
    "        outcomes = preprocessed_df['outcome_group']\n",
    "\n",
    "        # Rearrange columns\n",
    "        preprocessed_df = preprocessed_df[VAR_NUM + VAR_1HE + VAR_ELSE]\n",
    "\n",
    "        # One hot encoding province and country using SKLearn's 1 hot encoder\n",
    "        # We will use the infrequent_if_exist handle_unknown parameter to encode unseen categories as all zeros\n",
    "        encoder = OneHotEncoder(handle_unknown='infrequent_if_exist')\n",
    "        encoder.fit(preprocessed_df[['province', 'country']])\n",
    "        encoded = encoder.transform(preprocessed_df[['province', 'country']]).toarray()\n",
    "        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "        preprocessed_df = pd.concat([preprocessed_df, encoded_df], axis=1)\n",
    "        preprocessed_df = preprocessed_df.drop(columns=['province', 'country'])\n",
    "\n",
    "        # Iterative imputer to fill in missing values\n",
    "        imputer = IterativeImputer(random_state=42, n_nearest_features=5, max_iter=15)\n",
    "        imputer.fit(preprocessed_df)\n",
    "        imputed = imputer.transform(preprocessed_df)\n",
    "        preprocessed_df = pd.DataFrame(imputed, columns=preprocessed_df.columns)\n",
    "\n",
    "        # MinMaxScaler to scale the numerical columns\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(preprocessed_df[VAR_NUM])\n",
    "        scaled = scaler.transform(preprocessed_df[VAR_NUM])\n",
    "        scaled_df = pd.DataFrame(scaled, columns=VAR_NUM)\n",
    "\n",
    "        preprocessed_df = preprocessed_df.drop(columns=VAR_NUM)\n",
    "        preprocessed_df = pd.concat([preprocessed_df, scaled_df], axis=1)\n",
    "\n",
    "        preprocessed_df['outcome_group'] = outcomes\n",
    "\n",
    "        return preprocessed_df, encoder, imputer, scaler\n",
    "    \n",
    "    else:\n",
    "        #VAR_ELSE.remove('outcome_group')\n",
    "        # Rearrange columns\n",
    "        preprocessed_df = preprocessed_df[VAR_NUM + VAR_1HE + VAR_ELSE]\n",
    "\n",
    "        # One hot encoding province and country using the encoder\n",
    "        encoded = encoder.transform(preprocessed_df[['province', 'country']]).toarray()\n",
    "        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "        preprocessed_df = pd.concat([preprocessed_df, encoded_df], axis=1)\n",
    "        preprocessed_df = preprocessed_df.drop(columns=['province', 'country'])\n",
    "\n",
    "        # Impute missing values\n",
    "        imputed = imputer.transform(preprocessed_df)\n",
    "        preprocessed_df = pd.DataFrame(imputed, columns=preprocessed_df.columns)\n",
    "\n",
    "        # Scale numerical columns\n",
    "        scaled = scaler.transform(preprocessed_df[VAR_NUM])\n",
    "        scaled_df = pd.DataFrame(scaled, columns=VAR_NUM)\n",
    "\n",
    "        preprocessed_df = preprocessed_df.drop(columns=VAR_NUM)\n",
    "        preprocessed_df = pd.concat([preprocessed_df, scaled_df], axis=1)\n",
    "        \n",
    "        return preprocessed_df\n",
    "\n",
    "\n",
    "train, e, i, s = pipelinePreprocess2(train_o, locations, hospitalBeds, countryIncome, popDensity, True)\n",
    "test  = pipelinePreprocess2(test_o, locations, hospitalBeds, countryIncome, popDensity, False, e, i, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts 5,6, and 7 for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_rf2 = {\n",
    "    'n_estimators': Integer(10, 3000),\n",
    "    'max_depth': Integer(3, 30),\n",
    "    'min_samples_split': Integer(5, 30),\n",
    "    'min_samples_leaf': Integer(5, 30),\n",
    "    'max_features': Categorical(['auto', 'sqrt', 'log2']),\n",
    "    'class_weight': Categorical(['balanced', 'balanced_subsample', None]),\n",
    "    'ccp_alpha': Real(0.0, 0.1)}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = train.dropna()\n",
    "y_cols = X_cols['outcome_group']\n",
    "X_cols = X_cols.drop(columns=['outcome_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search 5 times using 50 iterations each time\n",
    "# Save the best model each time and the evaluation metrics such as macro f1 score, accuracy, and roc auc score, and f1 score for the deceased class\n",
    "\n",
    "best_models = []\n",
    "best_scores = []\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'f1': make_scorer(f1_score, average='macro'),\n",
    "           'f1_deceased': make_scorer(f1_score, labels=[0], average='macro'),\n",
    "           'f1_hospitalized': make_scorer(f1_score, labels=[1], average='macro'),\n",
    "           'f1_nonhospitalized': make_scorer(f1_score, labels=[2], average='macro')     \n",
    "           }\n",
    "\n",
    "for i in range(5):\n",
    "    search = BayesSearchCV(rf, param_dist_rf2, n_iter=15, random_state=dt.datetime.now().microsecond, cv=5, scoring=scoring, n_jobs=-1, verbose=2, refit='f1', return_train_score=True)\n",
    "    search.fit(X_cols, y_cols)\n",
    "    best_models.append(search.best_estimator_)\n",
    "    print(f'Best params: {search.best_params_}')\n",
    "\n",
    "    # Use the best model to perform 5 fold cross validation\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    scores = cross_validate(search.best_estimator_, X_cols, y_cols, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    print(f'Cross validation scores: {scores}')\n",
    "    best_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best models with the mean cross validation scores for macro f1, accuracy, and f1 score for the deceased class\n",
    "# Putting them in a dataframe and then saving as a text file\n",
    "\n",
    "best_models_df = pd.DataFrame(pd.Series(best_models))\n",
    "# Rename the column as Model\n",
    "best_models_df.columns = ['Model']\n",
    "best_scores_df = pd.DataFrame(best_scores)\n",
    "best_models_df['Mean Macro F1'] = best_scores_df['test_f1'].mean()\n",
    "best_models_df['Mean Accuracy'] = best_scores_df['test_accuracy'].mean()\n",
    "best_models_df['Mean F1 Deceased'] = best_scores_df['test_f1_deceased'].mean()\n",
    "\n",
    "# Sort by mean macro f1 score\n",
    "best_models_df = best_models_df.sort_values('Mean Macro F1', ascending=False)\n",
    "\n",
    "# Save df as text file\n",
    "best_models_df.to_csv('randomforest_tuning.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on the best f1 score\n",
    "best_model = best_models_df.iloc[0]['Model']\n",
    "\n",
    "# Store both training and testing metrics\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "\n",
    "# Perform 5 fold cross validation on the best model\n",
    "\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "kf.get_n_splits(X_cols, y_cols)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_cols, y_cols)):\n",
    "\n",
    "    X_train, X_test = X_cols.iloc[train_index], X_cols.iloc[test_index]\n",
    "    y_train, y_test = y_cols.iloc[train_index], y_cols.iloc[test_index]\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = best_model.predict(X_train)\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    # predict probabilities\n",
    "    y_pred_train_prob = best_model.predict_proba(X_train)\n",
    "    y_pred_test_prob = best_model.predict_proba(X_test)\n",
    "\n",
    "    train_metrics.append({'accuracy': accuracy_score(y_train, y_pred_train),\n",
    "                          'f1_macro': f1_score(y_train, y_pred_train, average='macro'),\n",
    "                          'f1_deceased': f1_score(y_train, y_pred_train, labels=[0], average='macro'),\n",
    "                          'f1_hospitalized': f1_score(y_train, y_pred_train, labels=[1], average='macro'),\n",
    "                          'f1_nonhospitalized': f1_score(y_train, y_pred_train, labels=[2], average='macro'),\n",
    "                          'roc_auc': roc_auc_score(y_train, y_pred_train_prob, multi_class='ovr')})\n",
    "\n",
    "    test_metrics.append({'accuracy': accuracy_score(y_test, y_pred_test),\n",
    "                          'f1_macro': f1_score(y_test, y_pred_test, average='macro'),\n",
    "                          'f1_deceased': f1_score(y_test, y_pred_test, labels=[0], average='macro'),\n",
    "                          'f1_hospitalized': f1_score(y_test, y_pred_test, labels=[1], average='macro'),\n",
    "                          'f1_nonhospitalized': f1_score(y_test, y_pred_test, labels=[2], average='macro'),\n",
    "                          'roc_auc': roc_auc_score(y_test, y_pred_test_prob, multi_class='ovr')})\n",
    "    \n",
    "train_metrics = pd.DataFrame(train_metrics)\n",
    "test_metrics = pd.DataFrame(test_metrics)\n",
    "\n",
    "train_metrics.mean(), test_metrics.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = RandomForestClassifier(class_weight='balanced_subsample', max_depth=30,\n",
    "                       max_features='auto', min_samples_leaf=5,\n",
    "                       min_samples_split=5, n_estimators=3000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best model on the entire training set\n",
    "best_model.fit(X_cols, y_cols)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = best_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "# Plotting the distribution of the train and test outcomes\n",
    "# Normalize the train and test outcome groups\n",
    "y_norm = y_cols.value_counts(normalize=True).sort_index().values\n",
    "y_pred_norm = pd.Series(y_pred).value_counts(normalize=True).sort_index().values\n",
    "\n",
    "# Plotting the normalized train and test outcome groups on the same plot\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar([0,1,2],y_norm, label='Train', alpha=0.5)\n",
    "plt.bar([0,1,2],y_pred_norm, label='Test', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('Train and Test Outcome Group')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/train_test_outcome_group.svg')\n",
    "plt.show()\n",
    "\n",
    "# Computing KL Divergence between the train and test outcome groups\n",
    "kl_div = entropy(pk=y_pred_norm, qk=y_norm)\n",
    "print(f'KL Divergence: {kl_div:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating submission file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission file\n",
    "create_submission_file(y_pred, 'submission_rf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cols, y_cols)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE()\n",
    "X_smote, y_smote = sm.fit_resample(X_cols, y_cols)\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "def f1_deceased(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None)[0]\n",
    "\n",
    "custom_scorer = make_scorer(f1_deceased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune hyperparameters\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters={\n",
    "'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "'hidden_layer_sizes': [(100,75), (100,50), (100,25),\n",
    "                       (150,100), (150,50), (100,75,50), \n",
    "                       (100,50,25),(150,75,50),(200,100,50),\n",
    "                      (200,150,100),(150,100,25),(150,125,100)],\n",
    "'alpha': [0.1,0.01,0.5,1,.001],\n",
    "'activation': [\"logistic\", \"relu\", \"tanh\", \"identity\"],\n",
    "'solver': [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "'learning_rate_init':[0.001,0.0001]\n",
    "}\n",
    "\n",
    "mlp = RandomizedSearchCV(MLPClassifier(), parameters,verbose=3,n_jobs=-1,n_iter=25,scoring={'f1_deceased': custom_scorer},refit='f1_deceased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X_cols,y_cols)\n",
    "mlp_best = mlp.best_estimator_ #show best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"f1_deceased: \",f1_score(y_test, mlp_best.predict(X_test),average=None)[0],\n",
    "      \"\\nf1_hospitalized: \",f1_score(y_test, mlp_best.predict(X_test),average=None)[1],\n",
    "      \"\\nf1_nonhospitalized: \",f1_score(y_test, mlp_best.predict(X_test),average=None)[2],\n",
    "\"\\nf1_macro: \",f1_score(y_test, mlp_best.predict(X_test), average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = train[train.columns[0:-1]]\n",
    "y_cols = train[train.columns[-1]]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_cols, y_cols, test_size=0.2, random_state=42, stratify=y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "X_smote = X_train\n",
    "y_smote = y_train\n",
    "\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "print(y_smote.value_counts())\n",
    "# Apply SMOTE to training data\n",
    "X_resampled, y_resampled = smote.fit_resample(X_smote, y_smote)\n",
    "\n",
    "# Check class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(pd.Series(y_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 'scale', 'auto'],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "svm_classifier = SVC(class_weight='balanced')\n",
    "\n",
    "print(\"Starting grid search\")\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "print(\"Grid searching\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Finished grid search\")\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_model_svm = grid_search.best_estimator_\n",
    "accuracy = best_model_svm.score(X_valid, y_valid)\n",
    "print(\"Accuracy on validation set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for cross-validation\n",
    "k = 5\n",
    "\n",
    "# Initialize variables to store evaluation metrics\n",
    "avg_accuracy = 0\n",
    "avg_f1 = 0\n",
    "avg_f1_deceased = 0\n",
    "avg_f1_hospitalized = 0\n",
    "avg_f1_nonhospitalized = 0\n",
    "\n",
    "# Initialize the cross-validator\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over each fold\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_resampled, y_resampled)):\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    X_train_cv, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "    y_train_cv, y_test = y_resampled.iloc[train_index], y_resampled.iloc[test_index]\n",
    "    \n",
    "    # Fit the SVM model on the training data\n",
    "    best_model_svm.fit(X_train_cv, y_train_cv)\n",
    "    \n",
    "    # Predict labels for the testing data\n",
    "    y_pred = best_model_svm.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics for this fold\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_scores = f1_score(y_test, y_pred, average=None)\n",
    "    \n",
    "    # Accumulate evaluation metrics for averaging\n",
    "    avg_accuracy += accuracy\n",
    "    avg_f1 += f1\n",
    "    avg_f1_deceased += f1_scores[0]\n",
    "    avg_f1_hospitalized += f1_scores[1]\n",
    "    avg_f1_nonhospitalized += f1_scores[2]\n",
    "    \n",
    "    print(f'Fold {i+1} - Accuracy: {accuracy:.4f} - F1: {f1:.4f} - F1 Deceased: {f1_score(y_test, y_pred, average=None)[0]}')\n",
    "\n",
    "# Calculate average evaluation metrics across all folds\n",
    "avg_accuracy /= k\n",
    "avg_f1 /= k\n",
    "avg_f1_deceased /= k\n",
    "avg_f1_hospitalized /= k\n",
    "avg_f1_nonhospitalized /= k\n",
    "\n",
    "print(f'Average across {k}-fold CV - Accuracy: {avg_accuracy:.4f} - F1: {avg_f1:.4f} - F1 Deceased: {avg_f1_deceased:.4f} - F1 Hospitalized: {avg_f1_hospitalized:.4f} - F1 Nonhospitalized: {avg_f1_nonhospitalized:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = best_model_svm.predict(X_train)\n",
    "y_val_pred = best_model_svm.predict(X_valid)\n",
    "\n",
    "# Calculate evaluation metrics for training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "# Calculate evaluation metrics for validation set\n",
    "val_accuracy = accuracy_score(y_valid, y_val_pred)\n",
    "val_precision = precision_score(y_valid, y_val_pred, average='weighted')\n",
    "val_recall = recall_score(y_valid, y_val_pred, average='weighted')\n",
    "val_f1 = f1_score(y_valid, y_val_pred, average='weighted')\n",
    "val_f1_per_class = f1_score(y_valid, y_val_pred, average=None)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Training Set Performance:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1-score: {train_f1:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-score: {val_f1:.4f}\")\n",
    "print(\"F1 Score per class: (in order deceased, hospitalized, nonhospitalized)\", val_f1_per_class)\n",
    "\n",
    "# Evaluate the model on the train set\n",
    "print('\\nTraining Report:\\n', classification_report(y_train, y_train_pred))\n",
    " \n",
    "# Evaluate the model on the test set\n",
    "print('\\nValidation report:\\n', classification_report(y_valid, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_val = confusion_matrix(y_valid, y_val_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm_train, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Training Set)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cm_val, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Validation Set)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
